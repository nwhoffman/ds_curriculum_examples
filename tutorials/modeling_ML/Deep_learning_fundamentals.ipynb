{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgLyM5quNN50"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In the previous module, we learned more about the main components of neural networks, including neurons, weights, activation function , and the types of layers. We also created a simple perceptron binary classifier and an actual neural network using the keras library. As we built those models, we focused more on the architecture of the models, such as the number of layers and neurons in thos layers. We also looked at parameters like the batch size and the learning rate and how they affect training time.\n",
    "\n",
    "Now we're going to get more specific about *how* a neural network learns. First, we need to specify that we're talking about a *feed-forward* neural network, where information moves forward from the input. Even though the weights are adjusted each iteration, this process doesn't transfer information from one layer to the next inside the neural network. \n",
    "\n",
    "In contrast, consider a multi-layer neural network where this transfer does occur. If we added feedback from the last hidden layer to the first hidden layer it would be considered a *recurrent neural network* (RNN), something we will cover in the next sprint.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "So, what's going one when we train a neural network? First, remember we adjust the weights by comparing the model prediction with the expected result, and either adding or subtracting to those values to get closer to the correct answer. To do that, we minimize a loss function that compares to the output to the target (correct answer). This function is minimized using the process of gradient descent.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Given a function, in this case the neural network loss function, we find the minimum of the function with the process of gradient descent. We start by choosing a random location on the function, find the direction of the negative gradient, and then repeat the process to find a minimum of the function. Sometimes, we find a local minimum but the goal is to find the global minumum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgUTRgkJNN-i"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Let's work through an example with a real function and find the minimum using gradient descent. The function we'll use is a simple parabola which has a single minimum.\n",
    "\n",
    "$$ y = (x-3)^2 $$\n",
    "\n",
    "Let's graph this function to see if we can see visually where the minimum is. \n",
    "\n",
    "<p><math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mi></mi><annotation encoding=\"application/x-tex\"></annotation></semantics></math></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "Ft40RmGGNIgB",
    "outputId": "2ae9e15d-6db8-4f34-c1dd-a6eda3a0d099",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(-10, 15, 0.01)\n",
    "y = (x-3)**2\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "\n",
    "plt.clf() # comment/delete to show plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VVms_jqKAPVQ"
   },
   "source": [
    "![mod2_obj1_function.png](https://raw.githubusercontent.com/LambdaSchool/data-science-canvas-images/main/unit_4/sprint_2/mod2_obj1_function.png)\n",
    "\n",
    "We can see it reaches a minimum (y=0) when x=3. Using calculus, we can find the minimum mathematically. We do this by looking slope of the function and finding when that slope is equal to zero. The slope is found by taking the derivative of the function. We'll take the derivative of $y$ with respect to $x$.\n",
    "\n",
    "$$ \\frac{dx}{dy} = 2(x-3)$$\n",
    "\n",
    "If we set the derivative equal to zero and solve for $x$, we'll have a solution for the minimum of the function:\n",
    "\n",
    "$$ 0 = 2(x-3)$$ which is true when $x = -3$.\n",
    "\n",
    "Using Python, we'll implement a gradient descent function. First let's rewrite the equation as follows:\n",
    "\n",
    "$$ x_{current} = (x_{previous} - \\text{learning rate}) * \\frac{dx}{dy}$$ where $\\frac{dx}{dy}$ equals $2(x_{previous}-3)$.\n",
    "\n",
    "\n",
    "The basic steps that we'll follow are:\n",
    "\n",
    "* initialize at a value for x\n",
    "* calculate a \"new\" current *x* with the learning rate and gradient\n",
    "* update the next iteration with this \"new\" value of x\n",
    "* iterate until the maximum number of iterations is reached\n",
    "\n",
    "<p><math display=\"inline\" xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mi></mi><annotation encoding=\"application/x-tex\"></annotation></semantics></math></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "Oo4g_fml7QS1",
    "outputId": "93fc821d-ebb8-438c-b44b-12dbf3e1b5a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - x value: 1.2.\n",
      "Iteration 2 - x value: 1.38.\n",
      "Iteration 3 - x value: 1.5419999999999998.\n",
      "Iteration 4 - x value: 1.6877999999999997.\n",
      "Iteration 5 - x value: 1.8190199999999999.\n",
      "Iteration 6 - x value: 1.937118.\n",
      "Iteration 7 - x value: 2.0434061999999997.\n",
      "Iteration 8 - x value: 2.1390655799999996.\n",
      "Iteration 9 - x value: 2.2251590219999997.\n",
      "Iteration 10 - x value: 2.3026431198.\n",
      "Iteration 11 - x value: 2.37237880782.\n",
      "Iteration 12 - x value: 2.4351409270380002.\n",
      "Iteration 13 - x value: 2.4916268343342.\n",
      "Iteration 14 - x value: 2.54246415090078.\n",
      "Iteration 15 - x value: 2.5882177358107024.\n",
      "Iteration 16 - x value: 2.629395962229632.\n",
      "Iteration 17 - x value: 2.6664563660066687.\n",
      "Iteration 18 - x value: 2.6998107294060016.\n",
      "Iteration 19 - x value: 2.7298296564654017.\n",
      "Iteration 20 - x value: 2.7568466908188616.\n",
      "Iteration 21 - x value: 2.7811620217369755.\n",
      "Iteration 22 - x value: 2.8030458195632777.\n",
      "Iteration 23 - x value: 2.82274123760695.\n",
      "Iteration 24 - x value: 2.840467113846255.\n",
      "Iteration 25 - x value: 2.8564204024616293.\n",
      "The local minimum occurs at 2.8564204024616293\n"
     ]
    }
   ],
   "source": [
    "# Initialize at x=1\n",
    "cur_x = 1 \n",
    "\n",
    "# Learning rate (how much to adjust x each iteration)\n",
    "rate = 0.05\n",
    "\n",
    "# Maximum number of iterations\n",
    "max_iters = 25\n",
    "\n",
    "# Initialize interation counter\n",
    "iters = 0\n",
    "\n",
    "# Gradient of the function\n",
    "grad = lambda x: 2*(x-3)\n",
    "\n",
    "while iters < max_iters:\n",
    "  # Set the previous x as the current\n",
    "  prev_x = cur_x\n",
    "\n",
    "  # Calculate the \"new\" current x with the gradient\n",
    "  cur_x = prev_x - (rate * grad(prev_x))\n",
    "  \n",
    "  # Advance the iteration counter\n",
    "  iters = iters+1\n",
    "  print(\"Iteration {} - x value: {}.\".format(iters, cur_x))\n",
    "\n",
    "# Print out the final result\n",
    "print(\"The local minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hWsxvsDsR54f"
   },
   "source": [
    "After 25 iterations, the value is starting to converge on the minimum of $x=3$. To speed up the convergence, we can adjust the learning rate or the size of the step we adjust $x$ by. We'll leave that up to you to complete as a challenge.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uNn8ggKeSkvG"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "Following the example above, try experimenting with the gradient descent algorithm. Specifically, you can change the learning rate and adjust the maximum number of iterations. How close can you get to the minimum value of the function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyqwXMgeADEE"
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "* [Khan Academy: Gradient and Directional Derivatives](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/why-the-gradient-is-the-direction-of-steepest-ascent)\n",
    "* [Implement a Gradient Descent in Python](https://towardsdatascience.com/implement-gradient-descent-in-python-9b93ed7108d1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "mod2_obj1_grad_desc.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
